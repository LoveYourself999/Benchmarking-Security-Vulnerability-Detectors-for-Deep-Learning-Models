# Benchmarking-Security-Vulnerability-Detectors-for-Deep-Learning-Models
Project Discription:

Static vulnerability detection that scans the source code of a system with static code analysis techniques to reveal software vulnerabilities has been widely used in both academic and industry to help find real vulnerabilities. Recently, many static vulnerability tools have been proposed to find vulnerabilities for deep learning frameworks. Although, experiment results show most of these tools are effective on finding real vulnerabilities in deep learning frameworks, a comprehensive comparison among them is not available, this is because most these tools either evaluate their performance on a unique DL frameworks which are not used by other tools or cover a unique set of APIs which also not cover by other detectors.   

This project will first conduct an empirical study to survey all the existing static vulnerability detection tools that are designed for deep learning frameworks and further collect the involved deep learning frameworks from these studies as the new benchmark dataset. Based on the findings from the empirical study, we will further conduct experiments to compare the performance of these tools. We hope our study can provide actionable guidelines for both academic research and industry users on the selection of static vulnerability detection tools when testing deep learning frameworks. 


[4080-W23-Shangen Chen-2.pdf](https://github.com/LoveYourself999/Benchmarking-Security-Vulnerability-Detectors-for-Deep-Learning-Models/files/10462893/4080-W23-Shangen.Chen-2.pdf)
